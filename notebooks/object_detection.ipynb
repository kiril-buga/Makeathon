{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-20T10:17:15.709800Z",
     "start_time": "2024-10-20T10:17:14.509273Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "# Import Environment Variables\n",
    "def set_tokens():\n",
    "    global HUGGINGFACEHUB_API_TOKEN\n",
    "    global GROQ_API_KEY\n",
    "    load_dotenv()\n",
    "    if os.getenv(\"HUGGINGFACEHUB_API_TOKEN\") is not None:\n",
    "        HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "        print(f\"Token is added\")\n",
    "    if os.getenv(\"GROQ_API_KEY\") is not None:\n",
    "        GROQ_API_KEY = os.environ[\"GROQ_API_KEY\"]\n",
    "    if (os.getenv(\"AZURE_OPENAI_ENDPOINT\") is not None\n",
    "            or os.getenv(\"AZURE_OPENAI_API_KEY\") is not None):\n",
    "        AZURE_OPENAI_ENDPOINT = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "        AZUER_OPENAI_API_KEY = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "    else:\n",
    "        raise Exception(\"No API Token Provided!\")\n",
    "\n",
    "\n",
    "set_tokens()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is added\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Image assessment",
   "id": "548387b978bae76a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T10:17:27.662712Z",
     "start_time": "2024-10-20T10:17:15.721772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import base64\n",
    "from PIL import Image\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from functions import use_huggingface_endpoint\n",
    "\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "def get_image_caption(image_path):\n",
    "    \"\"\"Generates a short caption for the provided image....\"\"\"\n",
    "    encoded_image = encode_image(image_path)\n",
    "\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    model_name = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "    # image_to_text = pipeline(\"image-to-text\", model=model_name) image_to_text(image)\n",
    "\n",
    "    prompt = [\n",
    "        AIMessage(content=\"You are a bot that is a technical expert at analyzing aircraft images.\"),\n",
    "        HumanMessage(content=[\n",
    "            {\"type\": \"text\", \"text\": \"Provide the model of the aircraft. Then, provide a list of all aircraft elements you see in the image.\"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image;base64,{encoded_image}\"\n",
    "                },\n",
    "            },\n",
    "        ])\n",
    "    ]\n",
    "\n",
    "    llm = use_huggingface_endpoint(model_name, 0.3)\n",
    "    response = llm.bind(max_tokens=1024).invoke(prompt)\n",
    "\n",
    "    return response.content\n",
    "\n",
    "\n",
    "image_path = \"../data/example_aircraft.png\"\n",
    "get_image_caption(image_path)"
   ],
   "id": "e1fc5b966e593272",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kiril\\PycharmProjects\\Makeathon\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is added\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Based on the ticket board and livery, the aircraft shown in the image is an Airbus A220. The image shows a jet bridge and an engine, which is modeled in the Airbus A220 family. Unfortunately, I cannot confirm with certainty the model of the aircraft.\\n\\n**Terms of Service**: I can analyze images of aircraft skin damage and tell you where it's coming from, but i cannot speculate where the rest of the aircraft is from.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Classification and structured output",
   "id": "3b8e22f45f40502c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T10:17:31.296897Z",
     "start_time": "2024-10-20T10:17:27.970387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "tagging_prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Extract the desired information from the following passage.\n",
    "\n",
    "Only extract the properties mentioned in the 'Classification' function.\n",
    "\n",
    "Passage:\n",
    "{input}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "class Classification(BaseModel):\n",
    "    sentiment: str = Field(..., enum=[\"happy\", \"neutral\", \"sad\"])\n",
    "    aggressiveness: int = Field(\n",
    "        ...,\n",
    "        description=\"describes how aggressive the statement is, the higher the number the more aggressive\",\n",
    "        enum=[1, 2, 3, 4, 5],\n",
    "    )\n",
    "    language: str = Field(\n",
    "        ..., enum=[\"spanish\", \"english\", \"french\", \"german\", \"italian\"]\n",
    "    )\n",
    "\n",
    "\n",
    "tagging_prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Extract the desired information from the following passage.\n",
    "\n",
    "Only extract the properties mentioned in the 'Classification' function.\n",
    "\n",
    "Passage:\n",
    "{input}\n",
    "\"\"\"\n",
    ")\n",
    "# LLM\n",
    "llm = ChatGroq(temperature= 0, model=\"llama-3.1-8b-instant\").with_structured_output(\n",
    "    Classification\n",
    ")\n",
    "\n",
    "chain = tagging_prompt | llm"
   ],
   "id": "7a8c6abf37899e13",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T10:17:32.726428Z",
     "start_time": "2024-10-20T10:17:31.306153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input = (\"Salut, comment Ã§a va ?\")\n",
    "chain.invoke({\"input\": input})"
   ],
   "id": "f12972b9d0979c71",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classification(sentiment='neutral', aggressiveness=1, language='french')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Object Detection",
   "id": "ad9253c08893c776"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T10:35:46.646965Z",
     "start_time": "2024-10-20T10:35:45.219362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "def detect_objects(image_path):\n",
    "    \"\"\"Detects objects in the provided image....\"\"\"\n",
    "    set_tokens()\n",
    "\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    encoded_image = encode_image(image_path)\n",
    "\n",
    "    model_name = \"facebook/detr-resnet-101\"\n",
    "    # Hugging Face API URL for DETR model\n",
    "    client = InferenceClient(\n",
    "        model_name,\n",
    "        token=HUGGINGFACEHUB_API_TOKEN,\n",
    "    )\n",
    "\n",
    "    response = client.object_detection(image_path)\n",
    "    return response\n",
    "\n",
    "detect_objects(image_path)\n"
   ],
   "id": "a43aca81bb4da1a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is added\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ObjectDetectionOutputElement(box=ObjectDetectionBoundingBox(xmax=964, xmin=0, ymax=496, ymin=132), label='airplane', score=0.999583899974823)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T10:42:46.241195Z",
     "start_time": "2024-10-20T10:42:37.241325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "def detect_objects(image_path):\n",
    "    \"\"\"Detects objects in the provided image....\"\"\"\n",
    "    set_tokens()\n",
    "\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    encoded_image = encode_image(image_path)\n",
    "\n",
    "    model_name = \"nvidia/segformer-b0-finetuned-ade-512-512\"\n",
    "    # Hugging Face API URL for DETR model\n",
    "    client = InferenceClient(\n",
    "        model_name,\n",
    "        token=HUGGINGFACEHUB_API_TOKEN,\n",
    "    )\n",
    "\n",
    "    response = client.object_detection(image_path)\n",
    "    return response\n",
    "\n",
    "image_path = \"../data/example_aircraft.png\"\n",
    "detect_objects(image_path)\n"
   ],
   "id": "4da946bf2e5de21f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is added\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ObjectDetectionOutputElement(box=None, label='building', score=1.0),\n",
       " ObjectDetectionOutputElement(box=None, label='sky', score=1.0),\n",
       " ObjectDetectionOutputElement(box=None, label='tree', score=1.0),\n",
       " ObjectDetectionOutputElement(box=None, label='grass', score=1.0),\n",
       " ObjectDetectionOutputElement(box=None, label='person', score=1.0),\n",
       " ObjectDetectionOutputElement(box=None, label='mountain', score=1.0),\n",
       " ObjectDetectionOutputElement(box=None, label='runway', score=1.0),\n",
       " ObjectDetectionOutputElement(box=None, label='airplane', score=1.0),\n",
       " ObjectDetectionOutputElement(box=None, label='flag', score=1.0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-27T07:50:16.655851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import functions\n",
    "\n",
    "image, detections = functions.detect_objects(image_path)\n",
    "\n"
   ],
   "id": "d288d5fce18f9a53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "291b65da61fcd785"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
